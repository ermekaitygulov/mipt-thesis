\documentclass{mipt-thesis-bs}
% Следующие две строки нужны только для biblatex. Для inline-библиографии их следует убрать.
%\usepackage{mipt-thesis-biblatex}
\usepackage[ruled,vlined]{algorithm2e}
%\addbibresource{example.bib}
\usepackage{graphicx}
\usepackage{subfigure}

\title{Обучение с подкреплением с применением экспертных демонстраций}
\author{Айтыгулов Э.}
\supervisor{Осипов Г.\,С.}
%\referee{Петров Д.\,Е.}       % требуется только для mipt-thesis-ms
\groupnum{М05-878б}
\faculty{Физтех-школа Прикладной Математики и Информатики}
\department{Кафедра системных исследований}

\begin{document}

\frontmatter
\titlecontents

\chapter{Введение}
Обучение с подкреплением - быстро прогрессирующая область машинного обучения, которая позволяет решать различные задачи управления в пространствах состояний большой размерности. Используя современные методы оптимизации и аппроксимации были получены внушительные результаты: в \cite{Agent-57} бот переигрывает человека в 57 играх Atari 2600; в \cite{OpenAI} обучили ботов способных обыграть команду профессиональных игроков в многопользовательской игре Dota2; бот обученный в \cite{DeepMind} смог получить статус <<GrandMaster>> в онлайн игре StarCraft2, а это значит, что он обыгрывает $99.8\%$ игроков по всему миру. Однако, несмотря на успехи, обучение с подкреплением имеет ряд проблем, из-за которых оно не может быть применено в широком круге задач. Одна из таких проблем - необходимость большого количества взаимодействий со средой и поведение агента в начале обучения, которое далеко от идеального . Все это не позволяет применять обучение с подкреплением на реальных или сложных задачах. Для решения этих проблем могут быть использованы экспертные демонстрации и симуляторы. Использование демонстраций и адаптация моделей, обученных в симуляторе, к реальному миру - одни из самых перспективных направлений.

Цель данной работы применение методов обучения с подкреплением в задаче управления робототехническим манипулятором в симулляторе. В работе представлено окружение \textit{RozumEnv}, реализованы алгоритмы обучения в нем и исследуются методы по работе с <<неидеальными>> демонстрациями, т.к. это может быть использовано для ускорения процесса обучения и уменьшения затрачиваемых ресурсов для моделей, работающих с изображениями в качестве входных данных, с помощью моделей использующих данные доступные в симуляторе. 
Данный подход может быть применен в задачах по переносу моделей из симулятора в реальность

В качестве базовых алгоритмов использовались асинхронный алгоритм Ape-X DQN и улучшенная версия Ape-X DDPG  \cite{apex}. Рабочая среда была реализована с использованием симулятора CoppeliaSim\footnote{https://www.coppeliarobotics.com/} и пакета PyREP\footnote{https://github.com/stepjam/PyRep}.

\mainmatter
\chapter{Обучение с подкреплением}

Обучение с подкреплением - способ обучения модели с помощью ее взаимодействия с некоторой окружающей средой, в которой она совершает действия, наблюдает ее состояния и получает положительный или отрицательный отклики (подкрепление). Алгоритмы обучения с подкреплением нацелены на увеличение суммарной награды.

\section{Марковский процесс принятия решений}
Для описания всей системы используется марковский процесс принятия решений (рис.\ref{mdp}). В каждый момент времени t=0, 1, 2, 3... агент, находясь в состоянии $s_t\in S$, может совершать некоторое $a_t\in A$, за что получает вознаграждение $R_t$ с вероятностью $P(R_{t+1}|s_t,a_t)$. В ответ на действие $a_t$ окружение переходит в некоторое состояние $s'_t$ с вероятностью $P(s_{t+1}|s_t,a_t)$. Функция распределения $P$, должна обладать следующим свойством:

$P(s_{t+1}, R_{t}|s_t,a_t,s_{t-1},a_{t-1}) = P(s_{t+1}, R_{t}|s_t,a_t)$

Это свойство называется Марковским свойством. Оно означает, что динамика среды полностью описывается функцией распределения $P$ и что состояние $s_t$ содержит всю информацию определяющую следующие состояния системы. Используя $P$ можно посчитать математическое ожидание величины $R$:

$r(s, a) = \mathbb{E}[R_{t} | s_t, a_t]=\sum_{R \sim P} R \sum_{s_{t+1} \in S} P(s_{t+1}, R | s_t, a_t)$

Модель поведения агента описывается вероятностным распределением $\pi$: в состоянии $s$ агент выбирает действие $a$ с вероятностью $\pi(a|s)$. Цель агента увеличение суммарной награды за эпизод. Во избежании расходимости ряда вводится дисконтирующий фактор $0 \leq\gamma\leq 1$:

$G_{t} = R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}$

Кроме ограничения величины $G$ дисконтирующий фактор $\gamma$ также определяет что важнее для агента: максимизировать сиюминутную награду (в случае $\gamma$ близкого к нулю) или кумулятивную (в случае $\gamma$ близкого к единице).

%рисунок МППР
\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.5\linewidth]{img/mdp.png}}
    \vspace{-0.2cm}
    \caption{Марковский процесс принятия решений}
    \label{mdp}
\end{figure*}

\section{Функции ценности}

Большинство алгоритмов для оценки или улучшения стратегии так или иначе используют функции ценности - функции, которые оценивают величину $G$ при условии некоторого состояния $s$ или некоторого действия $a$ в состоянии $s$ с учетом текущей стратегии:

Функция ценности состояния: $V_{\pi}(s_t) = \mathbb{E}_{\pi}[G_{t} | s_t]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | s_t]$ 
 
Функция ценности действия: $Q_{\pi}(s_t, a_t) = \mathbb{E}_{\pi}[G_{t} | s_{t}, a_{t}]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | s_{t}, a_{t}]$

$V_\pi$ и $Q_\pi$ оценивают математическое ожидание суммарного вознаграждения, полученного до конца эпизода, при условии текущего состояния $s_t$ или действия $a_t$ в состоянии $s_t$ при условии следования далее стратегии $\pi$. Из определения функций ценности следуют следующие равенства (уравнения Беллмана):
\begin{center}
(1.1) $V_{\pi}(s_t) = \mathbb{E}_{\pi}[G_t | s_t]=\mathbb{E}_{\pi}[R_{t} + \gamma G_{t+1}| s_t] = \mathbb{E}_{\pi}[R_{t} + \gamma V_{\pi}(s_{t+1})| s_t]$ 

(1.2) $Q_{\pi}(s_t, a_t) = \mathbb{E}_{\pi}[G_{t} | s_t, a_t]=\mathbb{E}_{\pi}[R_{t} + \gamma G_{t+1}| s_t, a_t] = \mathbb{E}_{\pi}[R_{t} + \gamma V_{\pi}(s_{t+1})| s_t, a_t]$
\end{center}

Решение задачи обучения с подкреплением подразумевает поиск стратегии, которая получает положительных откликов среды больше какой либо другой стратегии. Функция ценности $V$ позволяет сравнить две стратегии:

\begin{center}
    $\pi \geq \pi' \Rightarrow \forall s \in S \hookrightarrow V_\pi(s) \geq V_{\pi'}$
\end{center}

$Q^*$ и $V^*$ обозначают функции ценности соответствующие $\pi^*= argmax_{\pi} V_\pi$

\section{Q-обучение}

В данной работе использовались два алгоритма обучения с подкреплением: DQN \cite{dqn} и TD3 \cite{td3}. Первый алгоритм работает с дискретным пространством действий, а второй с непрерывным. Оба алгоритма основаны на табличном методе поиска оптимальной стратегии - Q-обучение \cite{Q-learning}. Алгоритм представляет из себя итеративный процесс, который в МППР с конечным множеством $S$ и конечным множеством $A$ находит оптимальную стратегию $\pi^*$. Идея подхода лежит в использовании уравнений Беллмана для оптимальной стратегии:
\begin{center}
$Q_{\pi^*}(s_t, a_t) = \mathbb{E}_{\pi^*}[R_{t} + \gamma V_{\pi^*}(s_{t+1})| s_t, a_t]$

$V_{\pi^*}(s_t) = \max_{a\in A}Q_{\pi^*}(s_t, a)$
\end{center}

Грубо говоря, уравнения Беллмана значат, что если известна функция $Q_{\pi^*}$, то, чтобы стратегия была оптимальной, нужно выбирать действия с наибольшим значением $Q_{\pi^*}$ в каждом состоянии. Q-обучение использует этот факт для обновления стратегии, для которой на каждом шаге оценивает функцию $Q$ с помощью динамического программирования: 

\begin{algorithm}[H]
\SetAlgoLined
Initialize $Q(s, a)$ with zeros\;
 \While{current episode < max_episode}{
  Initialize $s$\;
  Initialize done=False\;
  \While{not done}{
  Choose $a$ for $s$ using $Q$ (e.g. $\epsilon$-greedy)\;
  Act $a$, observe $R, s', done$\;
  $Q(s, a) \leftarrow Q(s, a)+\alpha[R+\gamma \max _{a'} Q(s', a')-Q(s, a)]$\;
  $s \leftarrow s'$\;
  }
 }
\caption{Q-обучение}
\end{algorithm}

На каждом шаге действие выбирается с учетом текущего значения $Q$ и стратегии исследования, которая необходима, чтобы не сойтись к суб-оптимальному решению. В большинстве случаев используется $\epsilon$-жадная стратегия: с вероятностью $\epsilon$ выбирается случайное действие, а в остальных случаях $argmax_aQ(s,a)$. Разница $R+\gamma \max _{a'} Q(s', a')-Q(s, a)$ называется TD-ошибкой.

\section{Использование демонстраций}

Одним из важных направлений обучения с подкреплением является использование демонстраций. Это обусловлено тем, что использование демонстраций может облегчить и существенно ускорить обучение. Необходимость в их использовании может быть связана либо с неприемлемостью случайного поведения (например, когда обучение происходит не в симуляторе), либо со сложностью задачи (например, когда награда может быть получена только в некоторых состояниях и необходимо долго исследовать окружение в поисках этой награды).

%рисунок демонстраций
\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.3\linewidth]{img/demo.png}}
    \vspace{-0.2cm}
    \caption{Экспертные демонстрации}
    \label{demo}
\end{figure*}

Демонстрации представляют из себя набор траекторий (рис.\ref{demo}), и при этом в зависимости от постановки задачи некоторая информация может отсутствовать (например, могут быть записаны только состояния без действий или наград). Траектории записываются экспертом, при этом стратегия эксперта необязательно должна быть оптимальной. Также демонстрации могут быть записаны в другом МППР (например, может происходить дискретизация непрерывного множества действий), что также влияет на их качество.

Демонстрации могут использоваться для обучения с учителем, как это сделано в \cite{dqfd}, или для изменения МППР, как это сделано в \cite{pofd}. Также демонстрации могут быть использованы для обратного обучения с подкреплением, чтобы восстановить функцию наград $r$. Такой подход использовался в \cite{gail}.   

\chapter{Обзор Работ}

Задача захвата предмета является одной из фундаментальных среди задач по управлению робототехническим манипулятором. Способ захвата может сильно варьироваться в зависимости от формы объекта, типа объекта (деформируемый или не-деформируемый), положения объекта в пространстве (далеко/близко/под другим предметом и т.д.). Методы захвата условно могут быть разделены на две категории: аналитические методы и методы использующие данные для обучения модели. 

Аналитические методы используют стерео сенсоры или сенсоры глубины для анализа формы предмета, для выбора положения захвата и степени сжатия манипулятора. До 2000 года аналитические методы доминировали в данной области.

С развитием вычислительных возможностей появилась возможность строить модели обладающие большей обобщающей способностью, позволяя работать с необработанными изображениями или предметами, которые модель раньше не видела. Более подробный анализ методов произведен в \cite{survey}.

Алгоритмы обучения с подкреплением можно отнести к второй группе и они были успешно использованы в ряде работ. Однако методы обучения с подкреплением требуют большого количества проб и ошибок. Например, в \cite{levine} для обучения модели потребовался датасет, который собирался 2 месяца с помощью 14 роботов. Одно из направлений, которое нацелено на решение этой проблемы: трансфер моделей из симулятора в реальность. 
Симуляторы могут предоставить более быстрый набор данных, но из-за визуального различия, неточности физики и т.д., для качественного переноса модели требуются специальные техники. В работах \cite{abbel, james1, james2} это делается с помощью многоуровневой рандомизации окружения: в данных варьировалось освещение, формы предметов, текстуры, цвета и т.д.. В более сложных задачах этого оказывается недостаточно. Например, в \cite{0.35}, где модель обучалась строить башню из кубиков, процент успеха составлял 35\%. Для улучшения переноса используются техники домен адаптации \cite{dann}, генеративные модели \cite{gann} или вариации TCN \cite{tcn}. Пример использования может быть найден в работах \cite{danngann, jamesgann,  aytar} 

Обучение моделей по изображениям может быть ускорено в симуляторе за счет доступа к данными об окружении, которые в реальности не доступны. Это может быть положение руки, маска изображения конкретного предмета, координаты предмета и т.д. (эти данные не подаются напрямую модели принимающей решения). В работах \cite{james2, 0.35} используется архитектура асимметричного актор-критика, предложенная в \cite{assym-ac}, в которой актору доступно только изображение, а критику полное состояние. 
Авторы \cite{jamesgann} используют маски предметов для дополнительной задачи сегментации, которая улучшает визуальный кодировщик, используемый для принятия решений. 
Третий вариант использования возможностей симулятора: обучение модели на данных об окружении, чтобы использовать ее в качестве эксперта для обучения модели работающей с изображениями. Такой подход использовался в работах \cite{aytar, bcfeat}, и как показано в \cite{aytar, abbel2} экспертные демонстрации вносят в решение некоторых задач большой вклад, без которого модель может не обучиться.
Также вектор данных об окружении занимает меньше памяти по сравнению с изображениями, что важно для многих алгоритмов обучения с подкреплением, т.к. они требуют большого количества данных (порядка 1 миллиона сэмплов).
Но несмотря на плюсы, модели обученные в таком пространстве состояний имеют некоторые ограничения. Например, они не способны работать с предметами разной формы, т.к. геометрию произвольного предмета можно передать только в виде картинки.

Проблема такого рода может быть решена с использованием методов по работе с <<неидеальными>> демонстрациями: вектор-модель, обученная работать с одним видом предметов, может быть использована для записи демонстраций с произвольными предметами и далее использоваться методами, которые допускают невысокое качество экспертных данных. Метод \textit{ForgER}, описанный в работе представляет из себя модификацию алгоритма \cite{dqfd}, которая учитывает возможность <<неидеальных>> демонстраций. В работе проводится сравнение с оригинальным подходом и с алгоритмом \cite{pofd}, который также создан для работы с <<неидеальными>> демонстрациями и показал результаты намного выше базовых алгоритмов по работе с демонстрациями \cite{gail, dqfd} 


% \let\clearpage\relax 
\chapter{Используемые методы}
\section{Глубинное Q-обучение}

Большинство задач имеют непрерывное пространство $S$ большой размерности, что делает классический вариант Q-обучения непригодным. Однако современные вычислительные возможности и методы оптимизации позволяют вместо таблицы $Q(s,a)$ использовать аппроксиматор $Q_\theta$ с обучаемыми параметрами $\theta$. Например в работе \cite{dqn}, где такой подход использовался впервые, в качестве аппроксиматора выступает сверточная нейронная сеть. Ключевые элементы алгоритма, которые позволили использовать аппроксиматор: реплэй-буфер $D$ и таргет-копия аппроксиматора с параметрами $\theta'$, которые периодически заменялись параметрами $\theta$ основного аппроксиматора. Параметры $\theta$ настраивались с помощью оптимизации  целевого функционала $L$ одним из градиентных методов. В качестве $L$ может выступать среднеквадратичная ошибка:

\begin{center}
    
$L(\theta)=\frac{1}{N}\sum_{(s,a,r,s,d) \sim D} (Q_\theta(s, a) - target)$,

где $target = r + \gamma\cdot d \cdot Q_{\theta'}(s',argmax_{a'}Q_{\theta'}(s',a'))$,

$N$ - размер выборки.
\end{center}

\section{Стабилизация целевой переменной и оптимизация архитектуры аппроксиматора}

В работе \cite{double dqn} было показано, что проблема такого подхода в том, что целевая величина переоценивает значение $Q$. Авторы предлагают для стабилизации разделить выбор действия и оценку $Q$ с помощью таргет-копии:

\begin{center}
$target = r + \gamma\cdot d \cdot Q_{\theta'}(s',argmax_{a'}Q_{\theta}(s',a'))$ 
\end{center}

Также для уменьшения дисперсии в \cite{n-step} к TD-ошибке добавляют nTD-ошибку:

\begin{center}
$L_n(\theta)=\frac{1}{N}\sum_{(s,a,r,s,d) \sim D} (Q_\theta(s, a) - target_n)$,

где $target_n = \sum_{i=0}^{n}\gamma^i\cdot r_i + \gamma^n\cdot d_n \cdot Q_{\theta'}(s_n,argmax_{a_n}Q_{\theta'}(s_n,a_n))$,

$N$ - размер выборки.
\end{center}

Для ускорения сходимости авторы \cite{dueling dqn} предлагают, использовать величину $A(s,a)=Q(s,a) - V(s)$. 
В работе вместо одного аппроксиматора для величины $Q$ используются аппроксиматоры для $V$ и для $A$, значения которых далее суммируются. Такая архитектура позволяет не исследовать состояние $s$ и не тратить время на оценку $Q(s,a)$, если $s$ <<плохое>>.

\section{Приоритезированный буфер}

Реплэй буфер позволяет накапливать и пере-использовать опыт прошлых взаимодействий со средой. В \cite{dqn} элементы буфера могут быть использованы на текущей итерации с одинаковой вероятностью, но такой подход игнорирует разницу в сэмплах и не учитывает, что некоторые элементы могут быть намного важнее остальных, но в то же время могут быть очень редкими. В \cite{per} используют приоритизацию для ускорения обучения. Вероятность сэмпла тем больше, чем выше модуль ошибки модели на этом сэмпле.

Для сэмпла $i$ вводится величина $p_i=|\delta_i| + \epsilon$, где $\delta_i$ - ошибка модели, а $\epsilon$ - константа близкая к нулю, необходимая для ограничения $p_i$ снизу. Вероятность вычисляется по следующей формуле:

\begin{center}
    $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k} p_{k}^{\alpha}}$
\end{center}

С помощью константы $\alpha$ контролируется степень приоритизации: чем меньше $\alpha$, тем ближе вероятностное распределение к равномерному. Чтобы оценка математического ожидания соответствовала действительному распределению, каждый сэмпл входит в целевой функционал с обратно пропорциональным весом (Imortance Sampling):

\begin{center}
$w_{i}=\left(\frac{1}{N} \cdot \frac{1}{P(i)}\right)^{\beta}$    
\end{center}

Чем выше $\beta$, тем больший вклад делает каждый сэмпл. Одновременное увеличение $\beta$ и $\alpha$ увеличивает степень приоритизации

\section{Использование демонстраций}

В качестве расширения \cite{dqn}, которое бы позволило эффективно использовать демонстрации, был выбран алгоритм, предложенный в \cite{dqfd} с модифицированным буфером ForgER. Алгоритм комбинирует обучение с подкреплением и обучение с учителем и состоит из двух частей: пред-обучение на экспертных данных и дообучение в среде. Экспертные данные добавляются в буфер с более высоким приоритетом и остаются там до конца обучения. 

Т.к. в демонстрациях представлены только экспертные действия, то значения $Q$ известны тоже только для экспертных действий. Чтобы значения остальных действий не были произвольными используется добавка к целевому функционалу, которая ограничивает их значения:

\begin{center}
    $J_{E}(Q)=\max _{a \in A}[Q(s, a)+l(a_{E}, a)]-Q(s, a_{E})$,
    
    где $l(a_{E}, a) = 0$, если $a_{E} = a$, и константа $margin>0$ в ином случае
\end{center}
 
 
\section{ForgER}

В классическом алгоритме демонстрации не удаляются из буфера, но это не сказывается на обучении только в случае хороших демонстраций. На практике демонстрации могут быть записаны суб-оптимальной стратегией или записаны в других условиях и т.п.. Одним из таких примеров является изменение пространства действий: при решении некоторых задач производится дискретизация непрерывного пространства $A$, после которой итоговое пространство действий становится конечным, и т.о. дискретизация экспертных действий приводит к потери некоторой доли информации.  Это может привести к переобучению и суб-оптимальному поведению. По этой причине в буфере ForgER экспертные данные хранятся отдельно и добавлена возможность их удаления.

Скорость забывания контролируется параметром $d$ и пропорция демонстраций в выборке зависит номера текущего эпизода $k$:

\begin{center}
    $proportion_{demo} = \min(1, k/d)$
\end{center}

% \let\clearpage\relax
\section{TD3}

Для работы с непрерывным множеством действий $A$ широко используются алгоритмы основанные на детерминированном градиенте стратегии. В отличии от конечных множеств действий, где максимум аппроксимации $Q$ можно найти перебрав значения, в непрерывном множестве действий так сделать нельзя. Один из подходов для решения этой задачи - использование актор-критик архитектуры. В дополнении к апроксиматору $Q(s,a)$, который исполняет роль критика и параметризован весами $\theta^Q$, используется второй аппроксиматор $\mu:S\rightarrow A$, который исполняет роль актора и с помощью которого выбираются действия. Т.о. стратегия агента параметризуется весами $\theta^\mu$. Теорема о детерминированном градиенте стратегии позволяет оптимизировать $\theta^\mu$ с помощью градиентных методов. Целевым функционалом для оптимизации является $J(\mu)=\mathbb{E}_{s\sim P, a\sim\mu}G_0$ и теорема говорит, что градиент $J$ равен математическому ожиданию произведения градиента $Q$ по $\mu$ и градиента $\mu$:

\begin{center}
$\nabla_{\theta^\mu} J(\mu) =\mathbb{E}_{s \sim P}[\nabla_{\theta^\mu} \mu(s) \nabla_{a} Q^{a}(s, a)|_{a=\mu(s)}]$
\end{center}

На практике, чтобы обучение было более стабильным, также используются таргет-копии аппроксиматоров. В работе \cite{ddpg}, вместо периодического копирования весов используется усреднение Поляка:

\begin{center}
    $\theta^{\mu^{\prime}} \leftarrow \tau \theta^{\mu}+(1-\tau) \theta^{\mu^{\prime}}$
    
     $\theta^{Q^{\prime}} \leftarrow \tau \theta^{Q}+(1-\tau) \theta^{Q^{\prime}}$
\end{center}

\section{Сглаживание апроксимации функции значимости}

Проблема переоценки некоторых действий в начале обучения при использовании аппроксиматоров функции значимости также присутствует в актор-критик архитектуре. Если критик в состоянии $s$ имеет сильно выраженный максимум в некоторой точке $a\in A$, то этот максимум используется актором, что делает обучение более сложным. Данную проблему решают в работе \cite{td3}.

Во-первых частота обновлений параметров $\theta^\mu$ и параметров $\theta^{\mu'}, \theta^{Q'}$ таргет аппроксиматоров уменьшается по сравнению с частотой обновлений $\theta^Q$, чтобы TD-ошибка не аккумулировалась в таргет аппроксиматорах. Т.о задержка между обновлениями улучшает сходимость алгоритма.

Во-вторых оценка целевой переменной ограничивается сверху. Для этого используется второй аппроксиматор $Q_{twin}$, а к действиям добавляется гауссовский шум:

\begin{center}
    $a'(s')=\operatorname{clip}(\mu'(s')+\operatorname{clip}(\epsilon,-c, c), a_{L o w}, a_{H i g h}), \quad \epsilon \sim \mathcal{N}(0, \sigma)$
    
    $target=r+\gamma(1-d) \min _{i=1,2} Q_i(s', a'(s')$
\end{center}

\section{Параметризованный шум для эффективного исследования среды}

В работах \cite{ddpg, td3}, где пространство действий непрерывно, для исследования среды используется либо некоррелирующий гауссовский шум, либо коррелирующий гауссовский шум (случайный процесс Орнштейна-Уленбека), а в работах \cite{dqn, dueling dqn, double dqn, dqfd}, где пространство действий дискретно, используется $\epsilon$-жадная стратегия. Такие подходы не учитывают особенностей среды и их трудно настраивать. В работах \cite{noisy layers, adaptive noise} предлагаются адаптивные методы исследования, представленные в виде модифицированных слоев нейронной сети. Адаптивный метод исследования позволяет не настраивать параметры шума, т.к. веса аппроксиматора сами подстраиваются под него с учетом окружения, делая исследование более эффективным. 
В работе использовалась архитектура из \cite{noisy layers}. 

Классический слой нейронной сети $y$ вычисляется через произведение матрицы:

\begin{center}
    $y=w x+b$,
\end{center}

где $x$ - входной слой, а $w$ и $b$ настраиваемые параметры. Адаптивный шум добавляется к каждому слою следующим образом:

\begin{center}
    $y =(\mu^{w}+\sigma^{w} \odot \varepsilon^{w}) x+\mu^{b}+\sigma^{b} \odot \varepsilon^{b}$
\end{center}

Чтобы уменьшить время генерации случайных чисел, используется факторизированный шум: вместо генерации матрицы размером $p\times q$, где $p$ - размерность входного слоя, а $q$ - размерность выходного слоя, используются 2 случайных вектора $\varepsilon_i$ и $\varepsilon_j$ размерности $p$ и $q$. Матрица $\varepsilon^w$ и вектор $\varepsilon^b$ вычисляются следующим образом:

\begin{center}
$\varepsilon_{i j}^{w} =f(\varepsilon_{i}) f(\varepsilon_{j})$

$\varepsilon_{j}^{b} =f(\varepsilon_{j})$,

где $f(x)=\operatorname{sgn}(x) \sqrt{|x|}$
\end{center}

\section{Распределенные алгоритмы обучения с подкреплением}

Алгоритмам обучения с подкреплением для эффективного обучения требуется большое количество взаимодействий со средой, поэтому распределенные алгоритмы пользуются большой популярностью, т.к. при наличии вычислительных мощностей они позволяют ускорить набор необходимого опыта. В данной работе в качестве распределенной системы была выбрана архитектура Ape-X (рис.\ref{apex}) представленная в \cite{apex}.

Процесс обучения разбивается на 3 типа под-процессов: 1) <<Актор>> под-процессы, которые набирают опыт; 2) под-процесс <<Учитель>> , который оптимизирует параметры; 3) <<Буфер>> под-процесс, который добавляет накопленные данные в приоритизированый буфер, подготавливает их к обучению и обновляет приоритеты. Под-процессы работают асинхронно и при правильной настройке не ждут друг друга, в отличии от синхронных методов распределения. 

% рисунок Apex
\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.5\linewidth]{img/apex.png}}
    \vspace{-0.2cm}
    \caption{Архитектура Ape-X}
    \label{apex}
\end{figure*}

<<Актор>> под-процессы накапливают опыт, вычисляют приоритеты на основе текущей копии параметров и отправляют его <<Буфер>> под-процессу. Под-процесс <<Учитель>> обновляет параметры и периодически отправляют их <<Актор>> под-процессам. <<Буфер>> под-процесс принимает данные, обновляет приоритеты и подготавливает выборку для под-процесса <<Учитель>>.

В случае конечного множества действий $A$ для эффективного исследования агенты используют $\epsilon$-жадную стратегию с разными параметрами $\epsilon$: агенты с большим $\epsilon$ исследуют среду более широко, в то время как агенты с маленьким $\epsilon$ более глубоко.
В случае непрерывного множества действий использовался некоррелированный гауссовский шум.

 
% \let\clearpage\relax
\chapter{Эксперименты}

В этой секции описываются детали проведенных экспериментов. Были проведены эксперименты показывающие важность забывания в разных окружениях и эксперименты по обучению робототехнического манипулятора в задаче по захвату предмета в дискретном и непрерывном множествах действий с использованием данных из симулятора в качестве состояний. 


\section{Изучение вклада забывания в обучающий процесс}

Для изучения вклада забывания были рассмотрены следующие окружения:

1) сложная среда \textit{MineRLTreechop-v0} реализованная с помощью платформы для создания окружений в игре \textit{MineCraft} \textit{Malmo};

2) набор окружений \textit{ClassicControl}: \textit{MountainCar-v0, CartPole-v0, Acrobot-v1};

3) набор окружений \textit{Box2D}: \textit{LunarLander-v2, CarRacing-v0}.

Окружение \textit{MineRLTreechop-v0} было реализовано в рамках соревнования \textit{MineRL}, как одно из вспомогательных окружений. Вместе с окружениями были собраны демонстрации для каждого из них. В окружении \textit{MineRLTreechop-v0} цель агента научиться срубать блоки деревьев. За каждый блок агент получал $r=1$. Состояние - картинка размером 64x64. Пространство действий представляло из себя 8 действий, которые принимают значения \{0,1\} и два непрерывных действия по повороту камеры. Чтобы показать вклад забывания при изменении пространства действий были рассмотрены две дискретизации: 

1) с 7 действиями;

2) с 10 действиями - более аккуратная,

и 3 варианта забывания:

1) без забывания, с буфером предложенным в \cite{dqfd}

2) с частичным забыванием (минимальная пропорция демонстраций в выборке составляла 0.1)

3) с полным забыванием в течении 50 эпизодов

Как видно на рис.\ref{treechop}, чем больше меняется пространство действий, тем сильнее это сказывается на обучении. 

\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[trim={110px 20px 80px 50px},width=0.43\linewidth]{img/treechop/e2.pdf}}
    \hspace{0.08\linewidth}
    \subfigure{\includegraphics[trim={110px 20px 80px 50px},width=0.43\linewidth]{img/treechop/e1.pdf}}
    \vspace{-0.2cm}
    \caption{ \textbf{(Слева)} Средняя награда за эпизод при дискретизации с 7 действиями.
    \textbf{(Справа)} Средняя награда за эпизод при дискретизации с 10 действиями.
    \label{treechop}
    }
\end{figure*}

На наборе окружений \textit{ClassicControl} и \textit{Box2D} \textit{ForgER} был сравнен с алгоритмом \cite{pofd}, который показал хороший результат при работе с неидеальными демонстрациями. \textit{ClassicControl} - набор окружений с дискретными множествами действий и для каждого были собраны данные с низкими и высокими средними наградами. \textit{Box2D} - окружения с непрерывными множествами действий и данные имели высокую среднюю награду, но действия дискретизировались при записи. Для каждого окружения и набора данных был произведен поиск оптимального значения ключевых параметров и усреднение по 4 запускам. В первом случае ForgER показал сравнимые результаты на данных хорошего качества, и полностью уступил на данных плохого качества. Однако во втором случае, несмотря на демонстрации с большой средней наградой, дискретизация сильно сказалась на процессе обучения \cite{pofd}.  

\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.48\linewidth]{img/discretization-lander-carracing/lander.pdf}}
    \hspace{0.02\linewidth}
    \subfigure{\includegraphics[width=0.48\linewidth]{img/discretization-lander-carracing/car.pdf}}

    \vspace{-0.2cm}
    \label{fig:exp:discrete:pofd}
    \caption{Сравнение алгоритмов на \textit{LunarLander} (\textbf{слева}) и \textit{CarRacing} (\textbf{справа})  с экспертными данными после дискретизации.}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.48\linewidth]{img/classic-control/Acrobot-v1/a-e3.pdf}}
    \hspace{0.02\linewidth}
    \subfigure{\includegraphics[width=0.48\linewidth]{img/classic-control/Acrobot-v1/a-e1.pdf}}
    \vspace{-0.2cm}
    \label{fig:exp:abot}
    \caption{Сравнение алгоритмов на среде \textit{Acrobot-v1}}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.48\linewidth]{img/classic-control/CartPole-v0/c-e3.pdf}}
    \hspace{0.02\linewidth}
    \subfigure{\includegraphics[width=0.48\linewidth]{img/classic-control/CartPole-v0/c-e2.pdf}}
    \vspace{-0.2cm}
    \label{fig:exp:cpole}
    \caption{Сравнение алгоритмов на среде \textit{CartPole-v0}}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.48\linewidth]{img/classic-control/MountainCar-v0/m-e2.pdf}}
    \hspace{0.02\linewidth}
    \subfigure{\includegraphics[width=0.48\linewidth]{img/classic-control/MountainCar-v0/m-e1.pdf}}
    \vspace{-0.2cm}
    \label{fig:exp:mcar}
    \caption{Сравнение алгоритмов на среде \textit{MountainCar-v0}}
\end{figure*}


\section{Окружающая среда \textit{RozumEnv}}

Для экспериментов с моделью робототехнического манипулятора Rozum в симуляторе CoppeliaSim была реализована среда \textit{RozumEnv} (рис. \ref{rozum}). Сцена симуляции представляет из себя стол с манипулятором и предметом перед ним. Управление производится с помощью пакета PyREP, который в отличии от официального API предлагает более быстрое взаимодействие с языком программирования \textit{Python}. Окружение позволяет комбинировать разные пространства состояний: картинка/(картинка, положение манипулятора)/ (положение манипулятора, положение предмета). В качестве $r$ использовалась функция $tolerance$ из работы \cite{deepmind suite}, зависящей от расстояния манипулятора до предмета, и большой награды за захват предмета:
\begin{center}
$r(distance) = tolerance(distance, margin=0.25)/20 + \mathds{1}(grasped)\cdot 10$
\end{center}

Действия представляют из себя вектор углов \textit{joints} в сочленениях манипуляторов и действия по захвату \textit{ee}. Манипулятор производит сжатие если $ee\geq 0.5$ и разжатие если $ee< 0.5$. Действия в сочленениях выполнялись в течении 4 симуляционных шагов, а действия по сжатию и разжатию выполнялись до конца. Всего эпизод длился 200 симуляционных шагов, что представляет из себя 10 секунд (один симуляционный шаг составляет 0.05 секунд). В начале эпизода положение предмета выбирается случайным образом на площадке размером 20смx15см.

%картинка RozumEnv

\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.8\linewidth]{img/RozumEnv.png}}
    \vspace{-0.2cm}
    \caption{Окружающая среда \textit{RozumEnv}}
    \label{rozum}
\end{figure*}

\section{Эксперименты в окружении \textit{RozumEnv}}
Обучение происходило в комбинированном пространстве состояний (<<рука>>, <<предмет>>), т.к. это требует меньшего количества памяти и позволяет запустить большое количество <<Актор>> под-процессов.
Также решение задачи на данных из симулятора легче решения задачи с использованием изображений и модель обученная таким образом, может быть использована в качестве эксперта для модели использующей изображения.
Вместе с методами по работе с "неидеальными" демонстрациями такой подход представляет из себя более гибкую альтернативу подходу использованному в \cite{aytar}, т.к. грубое использование экспертных данных без учета их качества, несмотря на плюсы, может ограничить агента.  

Для обучения в окружении \textit{RozumEnv} были реализованы два алгоритма:

1) распределенная версия алгоритма Глубинного Q-обучения с модификациями по стабилизации из секции 3.2;

2) распределенная версия алгоритма TD3. 

Для применения алгоритма Глубинного Q-обучения была использована дискретизация и из всех возможных действий использовались только два поворота по/против часовой стрелки в каждом сочленении (12 действий) и 2 действия сжатия и разжатия манипулятора (всего 14 действий). В дополнение к $\epsilon$-жадной стратегии использовался параметризованный шум.   

В экспериментах с алгоритмом TD3 вместо некоррелирующего гауссовского шума использовался параметризованный шум и $\epsilon$-жадная стратегия для действия по сжатию манипулятора с разным $\epsilon$ для каждого агента.

Параллельно под-процессам набирающим опыт, также работал под-процесс тестирующий среду с $\epsilon=0$ (рис.\ref{dqntd3}).

%рис графиков
\begin{figure*}[ht]
    \centering
    \subfigure{\includegraphics[width=0.48\linewidth]{img/rozumenv/dqn.png}}
    \hspace{0.02\linewidth}
    \subfigure{\includegraphics[width=0.48\linewidth]{img/rozumenv/td3.png}}
    \vspace{-0.2cm}
    \caption{Графики обучения алгоритмов Ape-X DQN и Ape-X TD3}
    \label{dqntd3}
\end{figure*}

\section{Особенности реализации}

При работе с приоритезированным буфером сложность сэмплирования составляет $O(\operatorname{log}N)$, где $N$ - размер буфера, и в целях ускорения сэмплирования использовалась C++ реализация буфера, представленная в пакете \textit{cpprb}\footnote{https://ymd\_h.gitlab.io/cpprb/}.

Для реализации распределнной системы Ape-X \cite{apex} был использован пакет \textit{ray}\footnote{https://docs.ray.io/en/latest/} . Авторы \cite{apex} при реализации алгоритма для эффективной передачи данных между под-процессами и между GPU и CPU использовали класс \textit{FIFOQueue} из пакета \textit{tensorflow}\footnote{https://www.tensorflow.org/}. В данной работе использовалась новая версия пакета \textit{tensorflow} и новый интерфейс \textit{tensorflow.data}. Для эффективной передачи данных между GPU и CPU использовался метод $prefetch$, который позволяет производить обновления и передачу данных на GPU параллельно. Частота обновлений составляла $\sim85$ итераций в секунду при объеме выборки 256 сэмплов.

\backmatter

\chapter{Заключение}

В работе предложен метод для работы с <<неидеальными>> демонстрациями, окружение для обучения модели по управлению робототехническим манипулятором, а также распределенные алгоритмы решающие задачу захвата предмета в дискретном и непрерывном множествах действий. 

Предложенный метод был проверен на сложной среде \textit{MineRLTreechop-v0} и сравнен с другим алгоритмом на двух наборах сред и в разных постановках. Экспериментально было показано преимущество данного метода в случае демонстраций, в которых изменялось пространство действий, что приводило к потере информации.

Экспериментальная среда предоставляет возможность выбора пространства состояний, что может быть полезно при решении сложных задач. Одним из таких примеров является использование модели обученной в векторном пространстве состояний в качестве эксперта для модели обучающейся по изображениям. В дальнейших работах планируется комбинирование такого подхода вместе с методами для работы с <<неидеальными демонстрациями>>. Данный подход может оказаться методом ускоряющим обучение сложных моделей в симуляторах, однако предоставляющим больше гибкости по сравнению со схожими методами за счет учета качества демонстраций.  

% \printbib
% Следующие строки необходимо раскомментировать, а предыдущую закомментировать, если используется inline-библиография.
\begin{thebibliography}{99}
    \bibitem{dqn}
        Mnih V. et al., Humanlevel control through deep reinforcement learning. Nature, 2015.
518(7540):529–533.
    \bibitem{ddpg}
         T. P. Lillicrap et al., Continuous control with deep reinforcement
learning, arXiv preprint arXiv:1509.02971, 2015
    \bibitem{Agent-57}Puigdom{\`e}nech Badia, A., and 6 colleagues 2020.\ Agent57: Outperforming the Atari Human Benchmark.\ arXiv e-prints arXiv:2003.13350.
    \bibitem{OpenAI} OpenAI, and 26 colleagues 2019.\ Dota 2 with Large Scale Deep Reinforcement Learning.\ arXiv e-prints arXiv:1912.06680.
    \bibitem{DeepMind} Arulkumaran, K., Cully, A., Togelius, J.\ 2019.\ AlphaStar: An Evolutionary Computation Perspective.\ arXiv e-prints arXiv:1902.01724.
    \bibitem{apex} Horgan, D., and 6 colleagues 2018.\ Distributed Prioritized Experience Replay.\ arXiv e-prints arXiv:1803.00933.
    \bibitem{td3} Fujimoto, S., van Hoof, H., Meger, D.\ 2018.\ Addressing Function Approximation Error in Actor-Critic Methods.\ arXiv e-prints arXiv:1802.09477.
    \bibitem{Q-learning} C.J.C.H. Watkins. Learning from delayed rewards. 1989
    \bibitem{double dqn} van Hasselt, H., Guez, A., Silver, D.\ 2015.\ Deep Reinforcement Learning with Double Q-learning.\ arXiv e-prints arXiv:1509.06461.
    \bibitem{dueling dqn}  Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., de Freitas, N.\ 2015.\ Dueling Network Architectures for Deep Reinforcement Learning.\ arXiv e-prints arXiv:1511.06581.
    \bibitem{per} Schaul, T., Quan, J., Antonoglou, I., Silver, D.\ 2015.\ Prioritized Experience Replay.\ arXiv e-prints arXiv:1511.05952.
    \bibitem{n-step} Sutton, R. S. 1988. Learning to predict by the methods of
temporal differences. Machine learning 3(1):9–44
    \bibitem{dqfd} Hester, T., and 13 colleagues 2017.\ Deep Q-learning from Demonstrations.\ arXiv e-prints arXiv:1704.03732.
    \bibitem{noisy layers} Fortunato, M., and 11 colleagues 2017.\ Noisy Networks for Exploration.\ arXiv e-prints arXiv:1706.10295.
    \bibitem{adaptive noise} Plappert, M., and 8 colleagues 2017.\ Parameter Space Noise for Exploration.\ arXiv e-prints arXiv:1706.01905.
    \bibitem{pofd} Kang, B., Jie, Z. & Feng, J.. (2018). Policy Optimization with Demonstrations. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:2469-2478
    \bibitem{deepmind suite} Tassa, Y., and 11 colleagues 2018.\ DeepMind Control Suite.\ arXiv e-prints arXiv:1801.00690.
    \bibitem{gail} Ho, J., Ermon, S.\ 2016.\ Generative Adversarial Imitation Learning.\ arXiv e-prints arXiv:1606.03476.
    \bibitem{aytar} Jeong, R., and 7 colleagues 2019.\ Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation.\ arXiv e-prints arXiv:1910.09470.
    \bibitem{survey} Bohg, J., Morales, A., Asfour, T., Kragic, D.\ 2013.\ Data-Driven Grasp Synthesis - A Survey.\ arXiv e-prints arXiv:1309.2660.
    \bibitem{levine} Levine, S., Pastor, P., Krizhevsky, A., Quillen, D.\ 2016.\ Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.\ arXiv e-prints arXiv:1603.02199.
    \bibitem{abbel} Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., Abbeel, P.\ 2017.\ Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.\ arXiv e-prints arXiv:1703.06907.
    \bibitem{james1} James, S., Davison, A.~J., Johns, E.\ 2017.\ Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task.\ arXiv e-prints arXiv:1707.02267.
    \bibitem{james2} Matas, J., James, S., Davison, A.~J.\ 2018.\ Sim-to-Real Reinforcement Learning for Deformable Object Manipulation.\ arXiv e-prints arXiv:1806.07851.
    \bibitem{jamesgann} James, S., and 8 colleagues 2018.\ Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks.\ arXiv e-prints arXiv:1812.07252.
    \bibitem{dann} Ganin, Y., and 7 colleagues 2015.\ Domain-Adversarial Training of Neural Networks.\ arXiv e-prints arXiv:1505.07818.
    \bibitem{gann} Goodfellow, I.~J., and 7 colleagues 2014.\ Generative Adversarial Networks.\ arXiv e-prints arXiv:1406.2661.
    \bibitem{danngann}Bousmalis, K., and 11 colleagues 2017.\ Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping.\ arXiv e-prints arXiv:1709.07857.
    \bibitem{0.35} Zhu, Y., and 10 colleagues 2018.\ Reinforcement and Imitation Learning for Diverse Visuomotor Skills.\ arXiv e-prints arXiv:1802.09564.
    \bibitem{tcn} Sermanet, P., and 6 colleagues 2017.\ Time-Contrastive Networks: Self-Supervised Learning from Video.\ arXiv e-prints arXiv:1704.06888.
    \bibitem{assym-ac} Pinto, L., Andrychowicz, M., Welinder, P., Zaremba, W., Abbeel, P.\ 2017.\ Asymmetric Actor Critic for Image-Based Robot Learning.\ arXiv e-prints arXiv:1710.06542.
    \bibitem{bcfeat} Schwab, D., and 9 colleagues 2019.\ Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup.\ arXiv e-prints arXiv:1902.04706.
    \bibitem{abbel2}Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., Abbeel, P.\ 2017.\ Overcoming Exploration in Reinforcement Learning with Demonstrations.\ arXiv e-prints arXiv:1709.10089.
    
\end{thebibliography}

\end{document}
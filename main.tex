\documentclass{mipt-thesis-bs}
% Следующие две строки нужны только для biblatex. Для inline-библиографии их следует убрать.
\usepackage{mipt-thesis-biblatex}
\addbibresource{example.bib}

\title{Обучение с подкреплением с применением экспертных демонстраций}
\author{Айтыгулов Э.}
\supervisor{Осипов Г.\,С.}
%\referee{Петров Д.\,Е.}       % требуется только для mipt-thesis-ms
\groupnum{М05-878б}
\faculty{Физтех-школа Прикладной Математики и Информатики}
\department{Кафедра системных исследований}

\begin{document}

\frontmatter
\titlecontents

\chapter{Введение}

Цель данной работы применение методов обучения с подкреплением в задаче управления робототехническим манипулятором в симуляторе \cite{}. В работе предлагается метод ускорения процесса обучения и уменьшения затрачиваемых ресурсов для моделей, работающих с изображениями в качестве входных данных, с помощью моделей использующих данные доступные в симулляторе. 

Данный подход может быть применен в задачах по переносу моделей из симуллятора в реальность, когда модель обученная в симулляторе с помощью техник рандомизации или домен адаптации переносится на реального робота.

В качестве базовых алгоритмов использовались асинхронный алгоритм APEX DQN \cite{} и улучшенная версия APEX DDPG  \cite{}. Для симулляции окружающей среды использовался симуллятор Coppelia-Sim: с помощью стороннего пакета PyREP было реализованно окружение поддерживающее различные комбинации пространств состояний (картинка/(картинка, положение манипулятора)/ (положение, манипулятора, положение цели)

\chapter{Обзор Работ}



\mainmatter


\chapter{Обучение с подкреплением}

Обучение с подкреплением - способ обучения модели с помощью ее взаимодействия в некоторой окружающей среде, в которой она совершает действия, наблюдает ее состояния и получает положительный или отрицательный отклики (подкрепление). Алгоритмы обучения с подкреплением нацеленны на увеличение суммарной награды.
Используя современные методы оптимизации и аппроксимации были получены внушительные результаты: \cite{Agent-57} переигрывает человека в 57 играх Atari 2600; \cite{OpenAI}  обучили ботов способных обыграть команду профессиональных игроков в многопользовательской игре Dota2; бот обученный в \cite{DeepMind} смог получить статус "GrandMaster" в онлайн игре StarCraft2, т.е. он обыгрывает $99.8\%$ игроков. Однако несмотря на успехи обучение с подкреплением имеет ряд проблем, из-за которых оно не может быть применено в широком круге задач. Одна из таких проблем - необходимость большого количества взаимодействий со средой и далекое от идеального поведение агента во время этого взаимодействия. Все это не позволяет применять обучение с подкреплением на реальных или сложных задачах. Для решения этих проблем могут быть использованы экспертные демонстрации и симуляторы. Использование демонстраций и домен адаптация моделей, обученных в симуляторе - одни из самых перспективных направлений.

\section{Марковский процесс принятия решений}
Для описания всей системы используется марковский процесс принятия решений. В каждый момент времени t=0, 1, 2, 3... агент, находясь в состоянии $s_t\in S$ может совершать некоторое $a_t\in A$, за что получает вознаграждение $R_t$ с вероятностью $P(R_{t+1}|s_t,a_t)$. В ответ на действие $a_t$ окружение переходит в некоторое состояние $s'_t$ с вероятностью $P(s_{t+1}|s_t,a_t)$. Функция распределения $P$, должна обладать следующим свойством:

$P(s_{t+1}, R_{t}|s_t,a_t,s_{t-1},a_{t-1}) = P(s_{t+1}, R_{t}|s_t,a_t)$

Это свойство называется Марковским свойством. Оно означает, что динамика среды полностью описывается функцией распределения $P$ и что состояние $s_t$ содержит всю информацию определяющюю следующие состояния системы. Используя $P$ можно посчитать математическое ожидание величины $R$:

$r(s, a) = \mathbb{E}[R_{t} | s_t, a_t]=\sum_{R \sim P} R \sum_{s_{t+1} \in S} P(s_{t+1}, R | s_t, a_t)$

%определение МППР

%рисунок МППР

\section{Функции значимости}

В состоянии $s$ агент выбирает действие $a$ с вероятностью $\pi(a|s)$, где $\pi$ некоторое распределенние. Цель агента увеличение суммарной награды за эпизод. Во избежании расходимости ряда вводится дисконтирующий фактор $0 \leq\gamma\leq 1$:

G_{t} = R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}

Большинство алгоритмов используют функции значимости - функции, которые оценивают некоторое состояние $s$ или некоторое действие $a$ в состоянии $s$ с учетом текущей стратегии:

Функция значимости состояния: $v_{\pi}(s_t) = \mathbb{E}_{\pi}[G_{t} | s_t]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | s_t]$ 
 
Функция значимости действия: $q_{\pi}(s_t, a_t) = \mathbb{E}_{\pi}[G_{t} | s_{t}, a_{t}]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | s_{t}, a_{t}]$

Функция значимости оценивают математическое ожидание суммарной вознаграждения, полученного до конца эпизода, при условии текущего состояния $s_t$ или действия $a_t$ в со следования далее стратегии $\pi$

\chapter{DQN}
\section{Q-обучение}
\section{Стабилизация целевой переменной и оптимизация архитектуры нейронной сети}
\section{Приоритезированный буфер}


\chapter{TD3}
\section{Градиент стратегии}
\section{Исполнитель-Критик}
\section{Сглаживание апроксимации Q-функции}


\chapter{APEX}
\section{Архитектура}
\section{Исследование среды}

\chapter{Среда RozumEnv}


\backmatter

% \printbib
% Следующие строки необходимо раскомментировать, а предыдущую закомментировать, если используется inline-библиография.
%\begin{thebibliography}{99}
%    \bibitem{langmuir26}
%        H. Mott-Smith, I. Langmuir. ``The theory of collectors in gaseous discharges''. \emph{Phys. Rev.} \textbf{28} (1926)
%\end{thebibliography}

\chapter{Благодарности}

Благодарности идут тут.

\end{document}
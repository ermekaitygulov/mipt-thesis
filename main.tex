\documentclass{mipt-thesis-bs}
% Следующие две строки нужны только для biblatex. Для inline-библиографии их следует убрать.
\usepackage{mipt-thesis-biblatex}
\usepackage[ruled,vlined]{algorithm2e}
\addbibresource{example.bib}

\title{Обучение с подкреплением с применением экспертных демонстраций}
\author{Айтыгулов Э.}
\supervisor{Осипов Г.\,С.}
%\referee{Петров Д.\,Е.}       % требуется только для mipt-thesis-ms
\groupnum{М05-878б}
\faculty{Физтех-школа Прикладной Математики и Информатики}
\department{Кафедра системных исследований}

\begin{document}

\frontmatter
\titlecontents

\chapter{Введение}
Обучение с подкреплением - быстро прогрессирующая область машинного обучения, которая позволяет решать различные задачи управления в пространствах состояния большой размерности. Используя современные методы оптимизации и аппроксимации были получены внушительные результаты: \cite{Agent-57} переигрывает человека в 57 играх Atari 2600; \cite{OpenAI}  обучили ботов способных обыграть команду профессиональных игроков в многопользовательской игре Dota2; бот обученный в \cite{DeepMind} смог получить статус "GrandMaster" в онлайн игре StarCraft2, а это значит, что он обыгрывает $99.8\%$ игроков по всему миру. Однако, несмотря на успехи, обучение с подкреплением имеет ряд проблем, из-за которых оно не может быть применено в широком круге задач. Одна из таких проблем - необходимость большого количества взаимодействий со средой и поведение агента в начале обучение, которое далеко от идеального . Все это не позволяет применять обучение с подкреплением на реальных или сложных задачах. Для решения этих проблем могут быть использованы экспертные демонстрации и симуляторы. Использование демонстраций и адаптация моделей, обученных в симуляторе, к реальному миру - одни из самых перспективных направлений.

Цель данной работы применение методов обучения с подкреплением в задаче управления робототехническим манипулятором в симуляторе \cite{}. В работе предлагается метод ускорения процесса обучения и уменьшения затрачиваемых ресурсов для моделей, работающих с изображениями в качестве входных данных, с помощью моделей использующих данные доступные в симулляторе. 

Данный подход может быть применен в задачах по переносу моделей из симуллятора в реальность, когда модель, обученная в симулляторе, с помощью техник рандомизации или домен адаптации переносится на реального робота.

В качестве базовых алгоритмов использовались асинхронный алгоритм APEX DQN \cite{} и улучшенная версия APEX DDPG  \cite{}. Для симулляции окружающей среды использовался симуллятор Coppelia-Sim: с помощью стороннего пакета PyREP было реализованно окружение поддерживающее различные комбинации пространств состояний (картинка/(картинка, положение манипулятора)/ (положение, манипулятора, положение цели)

\chapter{Обзор Работ}



\mainmatter


\chapter{Обучение с подкреплением}

Обучение с подкреплением - способ обучения модели с помощью ее взаимодействия с некоторой окружающей средой, в которой она совершает действия, наблюдает ее состояния и получает положительный или отрицательный отклики (подкрепление). Алгоритмы обучения с подкреплением нацеленны на увеличение суммарной награды.

\section{Марковский процесс принятия решений}
Для описания всей системы используется марковский процесс принятия решений. В каждый момент времени t=0, 1, 2, 3... агент, находясь в состоянии $s_t\in S$ может совершать некоторое $a_t\in A$, за что получает вознаграждение $R_t$ с вероятностью $P(R_{t+1}|s_t,a_t)$. В ответ на действие $a_t$ окружение переходит в некоторое состояние $s'_t$ с вероятностью $P(s_{t+1}|s_t,a_t)$. Функция распределения $P$, должна обладать следующим свойством:

$P(s_{t+1}, R_{t}|s_t,a_t,s_{t-1},a_{t-1}) = P(s_{t+1}, R_{t}|s_t,a_t)$

Это свойство называется Марковским свойством. Оно означает, что динамика среды полностью описывается функцией распределения $P$ и что состояние $s_t$ содержит всю информацию определяющую следующие состояния системы. Используя $P$ можно посчитать математическое ожидание величины $R$:

$r(s, a) = \mathbb{E}[R_{t} | s_t, a_t]=\sum_{R \sim P} R \sum_{s_{t+1} \in S} P(s_{t+1}, R | s_t, a_t)$

Модель поведения агента описывается вероятностным распределением $\pi$. В состоянии $s$ агент выбирает действие $a$ с вероятностью $\pi(a|s)$. Цель агента увеличение суммарной награды за эпизод. Во избежании расходимости ряда вводится дисконтирующий фактор $0 \leq\gamma\leq 1$:

$G_{t} = R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}$

Кроме ограничения величины $G$ дисконтирующий фактор $\gamma$ также определяет что важнее для агента: максимизировать сиюминутную награду (в случае $\gamma$ близкого к нулю) или кумулятивную (в случае $\gamma$ близкого к единице).

%определение МППР

%рисунок МППР

\section{Функции ценности}

Большинство алгоритмов для оценки или улучшения стратегии так или иначе используют функции ценности - функции, которые оценивают величину $G$ при условии некоторого состояния $s$ или некоторого действия $a$ в состоянии $s$ с учетом текущей стратегии:

Функция ценности состояния: $V_{\pi}(s_t) = \mathbb{E}_{\pi}[G_{t} | s_t]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | s_t]$ 
 
Функция ценности действия: $Q_{\pi}(s_t, a_t) = \mathbb{E}_{\pi}[G_{t} | s_{t}, a_{t}]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | s_{t}, a_{t}]$

$V_\pi$ и $Q_\pi$ оценивают математическое ожидание суммарного вознаграждения, полученного до конца эпизода, при условии текущего состояния $s_t$ или действия $a_t$ в состоянии $s_t$ при условии следования далее стратегии $\pi$. Из определения функций ценности следуют следующие равенства (уравнения Беллмана):
\begin{center}
(1.1) $V_{\pi}(s_t) = \mathbb{E}_{\pi}[G_t | s_t]=\mathbb{E}_{\pi}[R_{t} + \gamma G_{t+1}| s_t] = \mathbb{E}_{\pi}[R_{t} + \gamma V_{\pi}(s_{t+1})| s_t]$ 

(1.2) $Q_{\pi}(s_t, a_t) = \mathbb{E}_{\pi}[G_{t} | s_t, a_t]=\mathbb{E}_{\pi}[R_{t} + \gamma G_{t+1}| s_t, a_t] = \mathbb{E}_{\pi}[R_{t} + \gamma V_{\pi}(s_{t+1})| s_t, a_t]$
\end{center}

Решение задачи обучения с подкреплением подразумевает поиск стратегии, которая получала положительных откликов среды больше какой либо другой стратегии. Функция ценности $V$ позволяет это сделать:

\begin{center}
    $\pi \geq \pi' \Rightarrow \forall s \in S \hookrightarrow V_\pi(s) \geq V_{\pi'}$
\end{center}

$Q^*$ и $V^*$ обозначают функции ценности соответствующие $\pi^*= argmax_{\pi} V_\pi$

\section{Q-обучение}

В данной работе использовались два алгоритма обучения с подкреплением: DQN \cite{DQN} и TD3 \cite{TD3}. Первый алгоритм работает с дискретным пространством действий, а второй с непрерывным. Оба алгоритма основаны на табличном методе поиска оптимальной стратегии - Q-обучение \cite{Q-обучение}. Алгоритм представляет из себя итеративный процесс, который в МППР с конечным множеством $S$ и конечным множеством $A$ находит оптимальную стратегию $\pi^*$. Идея подхода лежит в использовании уравнений Беллмана для оптимальной стратегии:
\begin{center}
$Q_{\pi^*}(s_t, a_t) = \mathbb{E}_{\pi^*}[R_{t} + \gamma V_{\pi^*}(s_{t+1})| s_t, a_t]$

$V_{\pi^*}(s_t) = \max_{a\in A}Q_{\pi^*}(s_t, a)$
\end{center}

Грубо говоря, уравнения Беллмана значат, что если известна функция $Q_{\pi^*}$, то, чтобы стратегия была оптимальной, нужно выбирать действия с наибольшим значением $Q_{\pi^*}$ в каждом состоянии. Q-обучение использует этот факт для обновления стратегии, для которой на каждом шаге оценивает функцию $Q$ с помощью динамического программирования: 

\begin{algorithm}[H]
\SetAlgoLined
Initialize $Q(s, a)$ with zeros\;
 \While{current episode < max_episode}{
  Initialize $s$\;
  Initialize done=False\;
  \While{not done}{
  Choose $a$ for $s$ using $Q$ (e.g. $\epsilon$-greedy)\;
  Act $a$, observe $R, s', done$\;
  $Q(s, a) \leftarrow Q(s, a)+\alpha[R+\gamma \max _{a'} Q(s', a')-Q(s, a)]$\;
  $s \leftarrow s'$\;
  }
 }
 \caption{Q-обучение}
\end{algorithm}

На каждом шаге действие выбирается с учетом текущего значения $Q$ и стратегии исследования, которая необходима, чтобы не сойтись к суб-оптимальному решению. В большинстве случаев используется $\epsilon$-жадная стратегия: с вероятностью $\epsilon$ выбирается случайное действие, а в остальных случаях $argmax_aQ(s,a)$. 

\chapter{DQN}

Большинство задач имеют непрерывное пространство $S$ большой размерности, что делает классический вариант Q-обучения непригодным. Однако современные вычислительные возможности и методы оптимизации позволяют вместо таблицы $Q(s,a)$ использовать аппроксиматор $Q_\theta$ с обучаемыми параметрами $\theta$. Например в работе \cite{DQN}, где такой подход использовался впервые, в качестве аппроксиматора выступает сверточная нейронная сеть. Ключевые элементы алгоритма, которые позволили использовать аппроксиматор: реплэй-буфер $D$ и таргет-копия аппроксиматора с параметрами $\theta'$, которые периодически заменялись параметрами $\theta$ основного аппроксиматора. Параметры $\theta$ настраивались с помощью оптимизации  целевого функционала $L$ одним из градиентных методов. в качестве $L$ может выступать среднеквадратичная ошибка:

\begin{center}
    
$L(\theta)=\sum_{(s,a,r,s,d) \sim D} (Q_\theta(s, a) - target)$,

где $target = r + \gamma\cdot d \cdot Q_{\theta'}(s',argmax_{a'}Q_{\theta'}(s',a'))$ 
\end{center}

\section{Стабилизация целевой переменной и оптимизация архитектуры нейронной сети}

В работе \cite{Double DQN} было показано, что проблема такого подхода в том, что целевая величина переоценивает значение $Q$. Авторы предлагают для стабилизации разделить выбор действия и оценку $Q$ с помощью таргет-копии:

\begin{center}
$target = r + \gamma\cdot d \cdot Q_{\theta'}(s',argmax_{a'}Q_{\theta}(s',a'))$ 
\end{center}

Для ускорения сходимости авторы \cite{Dueling DQN} предлагают, использовать величину $A(s,a)=Q(s,a) - V(s)$. В работе вместо одного аппроксиматора для величины $Q$ используются аппроксиматоры для $V$ и для $A$, значения которых далее суммируются. Такая архитектура позволяет не исследовать состояние $s$ и не тратить время на оценку $Q(s,a)$.

\section{Приоритезированный буфер}

Реплэй буфер позволяет накапливать и пере-использовать опыт прошлых взаимодействий со средой. Все элементы буфера могут быть использованы на текущей итерации с одинаковой вероятностью. Такой подход игнорирует разницу в сэмплах и не учитывает, что некоторые элементы могут быть намного важнее остальных, но в то же время могут быть очень редкими. В \cite{PER} используют приоритизацию для ускорения обучения. Вероятность сэмпла тем больше, чем выше модуль ошибки модели на этом сэмпле.

Для сэмпла $i$ вводится величина $p_i=|\delta_i| + \epsilon$, где $\delta_i$ - ошибка модели, а $\epsilon$ - константа близкая к нулю, необходимая для ограничения $p_i$ снизу. Вероятность вычисляется по следующей формуле:

\begin{center}
    $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k} p_{k}^{\alpha}}$
\end{center}

С помощью константы $\alpha$ контролируется степень приоритизации: чем меньше $\alpha$, тем ближе вероятностное распределение к равномерному. Чтобы оценка математического ожидания соответствовала действительному распределению, каждый сэмпл входит в целевой функционал с обратно пропорциональным весом (Imortance Sampling):

\begin{center}
$w_{i}=\left(\frac{1}{N} \cdot \frac{1}{P(i)}\right)^{\beta}$    
\end{center}

Чем выше $\beta$, тем больший вклад делает каждый сэмпл.

 \chapter{TD3}
\section{Градиент стратегии}
\section{Исполнитель-Критик}
\section{Сглаживание апроксимации Q-функции}


\chapter{APEX}
\section{Архитектура}
\section{Исследование среды}

\chapter{Среда RozumEnv}


\backmatter

% \printbib
% Следующие строки необходимо раскомментировать, а предыдущую закомментировать, если используется inline-библиография.
%\begin{thebibliography}{99}
%    \bibitem{langmuir26}
%        H. Mott-Smith, I. Langmuir. ``The theory of collectors in gaseous discharges''. \emph{Phys. Rev.} \textbf{28} (1926)
%\end{thebibliography}

\chapter{Благодарности}

Благодарности идут тут.

\end{document}